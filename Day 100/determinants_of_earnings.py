# -*- coding: utf-8 -*-
"""Determinants_of_Earnings.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16l9YbeTE-_cqr02ehRYJFlAtPokVmiKJ

# Introduction

The National Longitudinal Survey of Youth 1997-2011 dataset is one of the most important databases available to social scientists working with US data. 

It allows scientists to look at the determinants of earnings as well as educational attainment and has incredible relevance for government policy. It can also shed light on politically sensitive issues like how different educational attainment and salaries are for people of different ethnicity, sex, and other factors. When we have a better understanding how these variables affect education and earnings we can also formulate more suitable government policies. 

<center><img src=https://i.imgur.com/cxBpQ3I.png height=400></center>

### Upgrade Plotly
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install --upgrade plotly

"""###  Import Statements

"""

import pandas as pd
import numpy as np

import seaborn as sns
import plotly.express as px
import matplotlib.pyplot as plt

from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split

"""## Notebook Presentation"""

pd.options.display.float_format = '{:,.2f}'.format

"""# Load the Data


"""

df_data = pd.read_csv('NLSY97_subset.csv')

"""### Understand the Dataset

Have a look at the file entitled `NLSY97_Variable_Names_and_Descriptions.csv`. 

---------------------------

    :Key Variables:  
      1. S           Years of schooling (highest grade completed as of 2011)
      2. EXP         Total out-of-school work experience (years) as of the 2011 interview.
      3. EARNINGS    Current hourly earnings in $ reported at the 2011 interview

# Preliminary Data Exploration üîé

**Challenge**

* What is the shape of `df_data`? 
* How many rows and columns does it have?
* What are the column names?
* Are there any NaN values or duplicates?
"""

df_data.shape

"""## Data Cleaning - Check for Missing Values and Duplicates

Find and remove any duplicate rows.
"""

df_data.isna().values.any()

df_data.duplicated().values.any()

df_data = df_data.fillna(0)

df_data = df_data.drop_duplicates()

"""## Descriptive Statistics"""

df_data.columns

"""## Visualise the Features"""

df_data.describe()

"""# Split Training & Test Dataset

We *can't* use all the entries in our dataset to train our model. Keep 20% of the data for later as a testing dataset (out-of-sample data).  
"""

target = df_data['EARNINGS']  # this is the target variable, earnings
features = df_data.drop('EARNINGS', axis=1)  # to get all the other variables, we remove the earning column and now it represents our features

X_train, X_test, y_train, y_test = train_test_split(features, target, random_state=10, test_size=0.2)  # 0.2 = 20%

"""# Linear Regression with the Whole Dataset

Use sklearn to run the regression on the training dataset. How high is the r-squared for the regression on the training data?
"""

regression = LinearRegression()

regression.fit(X_train, y_train)

print(f"R-squared: {regression.score(X_train, y_train):.2}")

"""0.31 for R-squared is not very good, but it could be worse. Over 0.5 is considered good.

### Evaluate the Coefficients of the Model

Here we do a sense check on our regression coefficients. The first thing to look for is if the coefficients have the expected sign (positive or negative). 

Interpret the regression. How many extra dollars can one expect to earn for an additional year of schooling?
"""

regr_coef = pd.DataFrame(data=regression.coef_, index=X_train.columns, columns=['Coefficient'])
regr_coef

round(regr_coef.loc['S'].values[0], 2)  # values[0] gives the coefficient, answer given is $/hr

"""According to the above line of code, we can expect someone to earn $1.19/hour extra for each additional year of schooling.

### Analyse the Estimated Values & Regression Residuals

How good our regression is also depends on the residuals - the difference between the model's predictions ( ùë¶ÃÇ ùëñ ) and the true values ( ùë¶ùëñ ) inside y_train. Do you see any patterns in the distribution of the residuals? Check for skewness and mean as well.
"""

predicted_values = regression.predict(X_train)
residuals = (y_train - predicted_values)

residuals.shape  # very long data

# Original Regression of Actual vs. Predicted Earnings
plt.figure(dpi=100)
plt.scatter(x=y_train, y=predicted_values, alpha=0.6)
plt.plot(y_train, y_train, color='red')  # identity line, plotting the same variable against itself is equal to x=y
plt.title(f'Actual vs Predicted Earnings: $y _i$ vs $\hat y_i$', fontsize=17)
plt.xlabel('Actual Earnings dollars/hr $y _i$', fontsize=14)
plt.ylabel('Prediced Earnings dollars/hr $\hat y _i$', fontsize=14)
plt.show()

# Residuals vs Predicted values
plt.figure(dpi=100)
plt.scatter(x=predicted_values, y=residuals, alpha=0.6)
plt.plot(y_train, y_train, color='red')  # identity line
plt.title('Residuals vs Predicted Values', fontsize=17)
plt.xlabel('Predicted Earnings $\hat y _i$', fontsize=14)
plt.ylabel('Residuals', fontsize=14)
plt.show()

"""We want to look at the randomness of the residual graph and if the points form an approximately constant width band around the identity line.

Looking at this one, we can deduce:

- There is no linear pattern formed by the points, so the model most likely has no systemic bias.
- The width band is declining (heavier at the bottom of the identity line), which points to the variance not being constant. This isn't a surprise as R-squared is 0.31
- Normality is only an issue with small sample sizes, however we have many points of data here.
- The points do not seem to be independant, due to the cone shape of the spread, so we can assume there is autocorrelation.

"""

# Residual Distribution Chart
residual_mean = round(residuals.mean(), 2)
residual_skew = round(residuals.skew(), 2)

sns.displot(residuals, kde=True, color='indigo')
plt.title(f'Residuals Skew ({residual_skew}), Mean ({residual_mean})')
plt.show()

"""Mean of 0 is perfect, that is the ideal.
However, the skew is not 0, this means the data is biased to one side, very slightly in this case, by 2 to the right.

# Simple Linear Regression

Only use the years of schooling to predict earnings. Use sklearn to run the regression on the training dataset. How high is the r-squared for the regression on the training data?
"""

target2 = df_data['EARNINGS']  # this is the target variable, earnings (response variable)
features2 = df_data['S']  # use years of education as the feature variable (predictor variable)

X_train2, X_test2, y_train2, y_test2 = train_test_split(features, target, random_state=10, test_size=0.2)  # 0.2 = 20%

X_train2 = X_train2.array.reshape(-1, 1)  # we need to do this since our data only has a single feature, since linear regression expects a 2D array
X_test2 = X_test2.array.reshape(-1, 1)
y_train2 = y_train2.array.reshape(-1, 1)
y_test2 = y_test2.array.reshape(-1, 1)

regression2 = LinearRegression()

regression2.fit(X_train2, y_train2)

print(f"R-squared: {regression2.score(X_train2, y_train2):.2}")

"""This is a very bad value for r squared.

Evaluate the Coefficients of the Model
Here we do a sense check on our regression coefficients. The first thing to look for is if the coefficients have the expected sign (positive or negative).

Interpret the regression. How many extra dollars can one expect to earn for an additional year of schooling?
"""

round(regression2.coef_[0][0], 2)  # [0][0] since it is 1 value in the array, this is the coef for S

"""This means for every extra year of schooling, one can expect to earn 1.22 $/hr more.

Analyse the Estimated Values & Regression Residuals
How good our regression is also depends on the residuals - the difference between the model's predictions ( ùë¶ÃÇ ùëñ ) and the true values ( ùë¶ùëñ ) inside y_train. Do you see any patterns in the distribution of the residuals? Check for skewness and mean as well.
"""

predicted_values2 = regression2.predict(X_train2)
residuals2 = (y_train2 - predicted_values2)

residuals2.shape  # very long data

# Original Regression of Actual vs. Predicted Earnings
plt.figure(dpi=100)
plt.scatter(x=y_train2, y=predicted_values2, alpha=0.6)
plt.plot(y_train2, y_train2, color='red')  # identity line
plt.title(f'Actual vs Predicted Earnings: $y _i$ vs $\hat y_i$', fontsize=17)
plt.xlabel('Actual Earnings dollars/hr $y _i$', fontsize=14)
plt.ylabel('Prediced Earnings dollars/hr $\hat y _i$', fontsize=14)
plt.show()

# Residuals vs Predicted values
plt.figure(dpi=100)
plt.scatter(x=predicted_values2, y=residuals2, alpha=0.6)
plt.plot(y_train2, y_train2, color='red')  # identity line
plt.title('Residuals vs Predicted Values', fontsize=17)
plt.xlabel('Predicted Earnings $\hat y _i$', fontsize=14)
plt.ylabel('Residuals', fontsize=14)
plt.show()

"""The same analysis as the previous residual chart applies except, there is a linear upwards trend here which shows a clear systemic bias in the prediction model."""

# Residual Distribution Chart
residual_mean2 = round(residuals2.mean(), 2)
residual_skew2 = round(residuals2.skew(), 2)

sns.displot(residuals2.reshape(-1), kde=True, color='indigo')  # reshape needed since displot needs 1D array if it is 1,1 data
plt.title(f'Residuals Skew ({residual_skew2}), Mean ({residual_mean2})')
plt.show()

"""The mean is 0 which is ideal, however, the skew is even worse than compared to using all the variables in the dataset.

# Multivariable Regression

Now use both years of schooling and the years work experience to predict earnings. How high is the r-squared for the regression on the training data?
"""

target3 = df_data['EARNINGS']  # this is the target variable, earnings (response variable)
features3 = df_data[['S', 'EXP']]  # use years of education and work experience as the feature variable (predictor variable)

X_train3, X_test3, y_train3, y_test3 = train_test_split(features, target, random_state=10, test_size=0.2)  # 0.2 = 20%

regression3 = LinearRegression()

regression3.fit(X_train3, y_train3)

print(f"R-squared: {regression3.score(X_train3, y_train3):.2}")

"""Slightly better than single variable but still quite bad.

### Evaluate the Coefficients of the Model
"""

regr_coef3 = pd.DataFrame(data=regression3.coef_, index=X_train3.columns, columns=['Coefficient'])
regr_coef3

"""The increase for each additional year of school is dramatically larger than the single and full variable regressions, at 1.79 compared to 1.22 and 1.19 respectively. However, weirdly enough, more work experience correlates to a smaller increase, but an increase nonetheless in hourly wage.

### Analyse the Estimated Values & Regression Residuals
"""

predicted_values3 = regression3.predict(X_train3)
residuals3 = (y_train3 - predicted_values3)

residuals3.shape  # very long data

# Original Regression of Actual vs. Predicted Earnings
plt.figure(dpi=100)
plt.scatter(x=y_train3, y=predicted_values3, alpha=0.6)
plt.plot(y_train3, y_train3, color='red')  # identity line
plt.title(f'Actual vs Predicted Earnings: $y _i$ vs $\hat y_i$', fontsize=17)
plt.xlabel('Actual Earnings dollars/hr $y _i$', fontsize=14)
plt.ylabel('Prediced Earnings dollars/hr $\hat y _i$', fontsize=14)
plt.show()

# Residuals vs Predicted values
plt.figure(dpi=100)
plt.scatter(x=predicted_values3, y=residuals3, alpha=0.6)
plt.plot(y_train3, y_train3, color='red')  # identity line
plt.title('Residuals vs Predicted Values', fontsize=17)
plt.xlabel('Predicted Earnings $\hat y _i$', fontsize=14)
plt.ylabel('Residuals', fontsize=14)
plt.show()

"""Everything from the single variable residual plot applies to this one, but this one is very slightly better."""

# Residual Distribution Chart
residual_mean3 = round(residuals3.mean(), 2)
residual_skew3 = round(residuals3.skew(), 2)

sns.displot(residuals3, kde=True, color='indigo')
plt.title(f'Residuals Skew ({residual_skew3}), Mean ({residual_mean3})')
plt.show()

"""Mean is still 0, but somehow the skew has gotten even worse with the multivariable regression.

# Use Your Model to Make a Prediction

How much can someone with a bachelors degree (12 + 4) years of schooling and 5 years work experience expect to earn in 2011?
"""

# using multivariable (with 2 features) regression model
# [[S, EXP]], alternatively use dataframe to predict, no point in making one for 2 variables tho

prediction = regression3.predict([[16, 5]])[0]

print(f"${prediction:.4}/hour")

"""Quite low. :("""